{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCFace\n",
    "\n",
    "\n",
    "This is a simply guide that helped me to understand how arcface (https://arxiv.org/abs/1801.07698) and sphere-face (https://arxiv.org/abs/1704.08063) works.\n",
    "\n",
    "This guide will gently moves from softmax to arcface and sphere face.\n",
    "To better visualize what's going on MNIST dataset is used and a very simple architecture is used as a backbone.\n",
    "\n",
    "Have fun!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Checking if everything is alright with the GPU\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "from plot import plot_scatter\n",
    "\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple architecture used in our experiments\n",
    "\n",
    "def backbone(inputs, include_top=False, n_classes=10, training=None):\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1\")(inputs)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2), name=\"maxp1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name=\"batch_norm1\")(x, training=training)\n",
    "    x = tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\", name=\"conv2\")(x)\n",
    "    x = tf.keras.layers.MaxPool2D((2,2), name=\"maxp2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name=\"batch_norm2\")(x, training=training)\n",
    "    x = tf.keras.layers.Flatten(input_shape=x.shape[1:])(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name=\"fc1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name=\"batch_norm3\")(x, training=training)\n",
    "    x = tf.keras.layers.Dense(20, activation=None, name=\"embeddings\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x, training=training)\n",
    "    if include_top:\n",
    "        x = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Softmax cross entropy to ArcFace\n",
    "\n",
    "One of the work horses of Machine Learning is the Softmax + Cross Entropy loss.\n",
    "The softmax is defined as:\n",
    "\n",
    "\n",
    "$$\\text{soft}(x_i) = \\frac{ \\text{exp}(Wx_i + b) }{\\sum_{j=1}^n \\text{exp}(Wx_j + b)}  $$\n",
    "\n",
    "and the cross entropy loss is defined as:\n",
    "\n",
    "$$L_1 = -\\frac{1}{m}\\sum\\limits_{i=1}^m y_i \\text{log}(\\text{soft}(x_i)) $$\n",
    "\n",
    "Below follow an example using Tensorflow on how to train a CNN using this loss.\n",
    "What is important in this exercise is to observe the how the embedding space is organized in this 10 class classification problem.\n",
    "\n",
    "One of the main drawbacks for the cross-entropy loss for face representation is its hability to generate a face space discriminative enough in open set scenarious.\n",
    "It's possible to have a grasp of this in the t-SNE below where the representation for each one of the 10 classes is less \"visual\" compact (within class variability) compared with other examples .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 11s 6ms/step - loss: 0.2719 - accuracy: 0.9191\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0710 - accuracy: 0.9791\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0561 - accuracy: 0.9843\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0435 - accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0421 - accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0377 - accuracy: 0.9885\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0293 - accuracy: 0.9918\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0521 - accuracy: 0.9855\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0247 - accuracy: 0.9928\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0239 - accuracy: 0.9934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d932fa05ce42258499f1b5af9ba636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Loading MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "def softmax_cross_entropy(target, output, sparse=True, n_classes=10):\n",
    "    \n",
    "    target = tf.squeeze(target)\n",
    "    \n",
    "    ## SIMPLIFIED\n",
    "    \n",
    "    logits_max = tf.math.reduce_max(output, axis=-1, keepdims=True)\n",
    "    N = logits_max.shape[0]\n",
    "    shifted_logits = output - logits_max\n",
    "    exp_shifted_logits = tf.math.exp(shifted_logits)\n",
    "    \n",
    "    if sparse:\n",
    "        y = tf.one_hot(tf.cast(target,\"int32\"), depth=n_classes)\n",
    "    else:\n",
    "        y = target\n",
    "        \n",
    "    sum_exp = tf.math.reduce_sum(exp_shifted_logits,axis=-1, keepdims=True)\n",
    "    log_sum_exp = tf.math.log(sum_exp)\n",
    "    \n",
    "    sub = shifted_logits - log_sum_exp\n",
    "    mul = tf.math.multiply(tf.math.negative(y), sub)\n",
    "    #L = tf.math.reduce_sum(mul)/N\n",
    "    L = tf.math.reduce_mean(tf.math.reduce_sum(mul, axis=-1))\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    ### NOT SIMPLIFIED\n",
    "    logits_max = tf.math.reduce_max(output, axis=-1, keepdims=True)\n",
    "    shifted_logits = output - logits_max    \n",
    "    exp_shifted_logits = tf.math.exp(shifted_logits)\n",
    "    sum_exp = tf.math.reduce_sum(exp_shifted_logits,axis=-1, keepdims=True)\n",
    "    softmax = exp_shifted_logits/sum_exp\n",
    "\n",
    "    if sparse:\n",
    "        y = tf.one_hot(tf.cast(target,\"int32\"), depth=n_classes)\n",
    "    else:\n",
    "        y = target\n",
    "\n",
    "    mul = tf.math.multiply(y, tf.math.log(tf.clip_by_value(softmax,1e-10,1.0)))\n",
    "    #L = -tf.reduce_sum(mul)/N    \n",
    "    L = -tf.math.reduce_mean(tf.math.reduce_sum(mul, axis=-1))\n",
    "    \"\"\"\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"Input\")\n",
    "logits = backbone(inputs, include_top=True)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=softmax_cross_entropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "model.fit(train_dataset, epochs=10, steps_per_epoch=None)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_test[0:1000])\n",
    "plot_scatter(embeddings, y_test[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CenterLoss\n",
    "\n",
    "Centerloss addresses this representation issue from the cross-entropy by tackling explicitly the within-class variability of the face space (the layer where the embeddings will be taken).\n",
    "This is carried out by centering each class at a particular point in space.\n",
    "Then, the euclidean norm between a sample from one class and its center is minimized using the loss below.\n",
    "\n",
    "$$L_2 = 0.5 \\sum\\limits_{i=1}^{m}||x_i - x_{\\text{center }i} ||_{2}^{2} $$\n",
    "\n",
    "Normally, a face recognition deep model is trained jointly using $L_1$ and $L_2$.\n",
    "\n",
    "Below follow the same example as aforementioned, but using the center loss.\n",
    "It's possible to observe from the t-SNE below that the representation for each one of the 10 classes is more \"visual\" compact (within class variability) compared with the cross-entropy example .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"center_loss_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "maxp1 (MaxPooling2D)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_norm1 (BatchNormalizat (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "maxp2 (MaxPooling2D)         (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_norm2 (BatchNormalizat (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "batch_norm3 (BatchNormalizat (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "embeddings (Dense)           (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "centers (CenterLossLayer)    (None, 20)                200       \n",
      "_________________________________________________________________\n",
      "hot (Dense)                  (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 223,822\n",
      "Trainable params: 223,174\n",
      "Non-trainable params: 648\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5045: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.4554 - cross_entropy: 0.2903 - center_loss: 0.3302\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.6646 - cross_entropy: 0.3342 - center_loss: 0.6608\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.3847 - cross_entropy: 0.1987 - center_loss: 0.3719\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 1.2203 - cross_entropy: 0.5490 - center_loss: 1.3424\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2038 - cross_entropy: 0.1375 - center_loss: 0.1326\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1750 - cross_entropy: 0.1160 - center_loss: 0.1178\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2094 - cross_entropy: 0.1205 - center_loss: 0.1777\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 1.6970 - cross_entropy: 0.6827 - center_loss: 2.0286\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1735 - cross_entropy: 0.1088 - center_loss: 0.1294\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.1368 - cross_entropy: 0.0847 - center_loss: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote/idiap.svm/user.active/tpereira/github/paper_notes/notes/ml/arcface/plot.py:13: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a711df9e5e7495d9bf3b51da5c813c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CenterLossLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_classes, n_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.centers = tf.Variable(\n",
    "            tf.zeros([n_classes, n_features]),\n",
    "            name=\"centers\",\n",
    "            trainable=False,\n",
    "            # in a distributed strategy, we want updates to this variable to be summed.\n",
    "            aggregation=tf.VariableAggregation.SUM,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # pass through layer\n",
    "        return tf.identity(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"n_classes\": self.n_classes, \"n_features\": self.n_features})\n",
    "        return config\n",
    "    \n",
    "class CenterLoss(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        centers_layer,\n",
    "        alpha=0.9,\n",
    "        update_centers=True,\n",
    "        name=\"center_loss\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.centers_layer = centers_layer\n",
    "        self.centers = self.centers_layer.centers\n",
    "        self.alpha = alpha\n",
    "        self.update_centers = update_centers\n",
    "\n",
    "    def call(self, sparse_labels, prelogits):\n",
    "        sparse_labels = tf.reshape(sparse_labels, (-1,))\n",
    "        centers_batch = tf.gather(self.centers, sparse_labels)\n",
    "        # the reduction of batch dimension will be done by the parent class\n",
    "        center_loss = tf.keras.losses.mean_squared_error(prelogits, centers_batch)\n",
    "\n",
    "        # update centers\n",
    "        if self.update_centers:\n",
    "            diff = (1 - self.alpha) * (centers_batch - prelogits)\n",
    "            updates = tf.scatter_nd(sparse_labels[:, None], diff, self.centers.shape)\n",
    "            # using assign_sub will make sure updates are added during distributed\n",
    "            # training\n",
    "            self.centers.assign_sub(updates)\n",
    "\n",
    "        return center_loss\n",
    "\n",
    "\n",
    "class CenterLossModel(tf.keras.Model):\n",
    "    def compile(\n",
    "        self,\n",
    "        cross_entropy,\n",
    "        center_loss,\n",
    "        loss_weights,\n",
    "        train_loss,\n",
    "        train_cross_entropy,\n",
    "        train_center_loss,\n",
    "        test_acc,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().compile(**kwargs)\n",
    "        self.cross_entropy = cross_entropy\n",
    "        self.center_loss = center_loss\n",
    "        self.loss_weights = loss_weights\n",
    "        self.train_loss = train_loss\n",
    "        self.train_cross_entropy = train_cross_entropy\n",
    "        self.train_center_loss = train_center_loss\n",
    "        self.test_acc = test_acc\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, prelogits = self(images, training=True)\n",
    "            loss_cross = self.cross_entropy(labels, logits)\n",
    "            loss_center = self.center_loss(labels, prelogits)\n",
    "            loss = (\n",
    "                loss_cross * self.loss_weights[self.cross_entropy.name]\n",
    "                + loss_center * self.loss_weights[self.center_loss.name]\n",
    "            )\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)        \n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_cross_entropy(loss_cross)\n",
    "        self.train_center_loss(loss_center)\n",
    "        return {\n",
    "            m.name: m.result()\n",
    "            for m in [self.train_loss, self.train_cross_entropy, self.train_center_loss]\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images, labels = data\n",
    "        logits, prelogits = self(images, training=False)\n",
    "        self.test_acc(accuracy_from_embeddings(labels, prelogits))\n",
    "        return {m.name: m.result() for m in [self.test_acc]}\n",
    "    \n",
    "    \n",
    "\n",
    "# Loading MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = y_train.astype(\"int32\")\n",
    "y_test = y_test.astype(\"int32\")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"Input\")\n",
    "\n",
    "embeddings = backbone(inputs, include_top=False)\n",
    "embeddings = CenterLossLayer(\n",
    "        n_classes=n_classes, n_features=embeddings.shape[-1], name=\"centers\"\n",
    "    )(embeddings)\n",
    "\n",
    "logits = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(embeddings)\n",
    "\n",
    "\n",
    "model = CenterLossModel(\n",
    "    inputs=inputs, outputs=[logits, embeddings]\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, name=\"cross_entropy\"\n",
    ")\n",
    "center_loss = CenterLoss(\n",
    "    centers_layer=model.get_layer(\"centers\"),\n",
    "    alpha=0.9,\n",
    "    name=\"center_loss\",\n",
    ")\n",
    "\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"loss\")\n",
    "train_cross_entropy = tf.keras.metrics.Mean(name=\"cross_entropy\")\n",
    "train_center_loss = tf.keras.metrics.Mean(name=\"center_loss\")\n",
    "test_acc = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    cross_entropy=cross_entropy,\n",
    "    center_loss=center_loss,\n",
    "    loss_weights= {\"cross_entropy\": 1.0, \"center_loss\": 0.5},\n",
    "    train_loss=train_loss,\n",
    "    train_cross_entropy=train_cross_entropy,\n",
    "    train_center_loss=train_center_loss,\n",
    "    test_acc=test_acc,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_test[0:1000])\n",
    "plot_scatter(embeddings, y_test[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angular Boundaries\n",
    "\n",
    "## Modified Softmax from the paper Sphere-Face\n",
    "\n",
    "$$\\text{soft}(x_i) = \\frac{exp(||x_i||\\text{cos}(\\theta_{yi}))}{\\sum_j  exp(||x_i||\\text{cos}(\\theta_{j}))   }$$, where $cos(\\theta_i)=W_i^{\\intercal}x_i$\n",
    "\n",
    "Below follow the same example as aforementioned, but using the modified softmax.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2280: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_cross_entropy/Squeeze:0\", shape=(32,), dtype=uint8)\n",
      "Tensor(\"softmax_cross_entropy/one_hot:0\", shape=(32, 10), dtype=float32)\n",
      "Tensor(\"softmax_cross_entropy/Squeeze:0\", shape=(32,), dtype=uint8)\n",
      "Tensor(\"softmax_cross_entropy/one_hot:0\", shape=(32, 10), dtype=float32)\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2624 - accuracy: 0.9237\n",
      "Epoch 1/9\n",
      "Tensor(\"softmax_cross_entropy/Squeeze:0\", dtype=uint8)\n",
      "Tensor(\"softmax_cross_entropy/one_hot:0\", dtype=float32)\n",
      "Tensor(\"softmax_cross_entropy/Squeeze:0\", dtype=uint8)\n",
      "Tensor(\"softmax_cross_entropy/one_hot:0\", dtype=float32)\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 2.0980 - accuracy: 0.8935\n",
      "Epoch 2/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.7185 - accuracy: 0.9648\n",
      "Epoch 3/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6732 - accuracy: 0.9679\n",
      "Epoch 4/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6841 - accuracy: 0.9678\n",
      "Epoch 5/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.5654 - accuracy: 0.9736\n",
      "Epoch 6/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6129 - accuracy: 0.9716\n",
      "Epoch 7/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6615 - accuracy: 0.9697\n",
      "Epoch 8/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.6881 - accuracy: 0.9686\n",
      "Epoch 9/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.5821 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee1bf09a886491792d5501343f07bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class ModifiedSoftMaxHead(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_classes=10):\n",
    "        super(ModifiedSoftMaxHead, self).__init__(name =\"modified_softmax_logits\")\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(ModifiedSoftMaxHead, self).build(input_shape[0])\n",
    "        shape = [input_shape[-1], self.n_classes]\n",
    "        \n",
    "        self.W = self.add_variable(\"W\", shape=shape)\n",
    "        \n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        \n",
    "        # normalize feature\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        # cos between X and W            \n",
    "        cos_yi = tf.nn.l2_normalize(X, axis=1) @ W\n",
    "                \n",
    "        logits = tf.norm(X)*cos_yi\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "        \n",
    "tf.random.set_seed(0)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "\n",
    "# PRE MODEL WITH CROSS ENTROPY\n",
    "n_classes = 10\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"input\")\n",
    "labels = tf.keras.layers.Input([], name=\"label\")\n",
    "\n",
    "embeddings = backbone(inputs, include_top=False)\n",
    "logits_cross_entropy = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(embeddings)\n",
    "pre_model = tf.keras.Model(inputs=inputs, outputs=logits_cross_entropy)\n",
    "\n",
    "#### NOW THE MODIFIED CROSS ENTROPY\n",
    "\n",
    "logits_modsoft = ModifiedSoftMaxHead()(embeddings)\n",
    "\n",
    "modsoft_model = tf.keras.Model(inputs, outputs=logits_modsoft)\n",
    "#print(arcface_model.summary())\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# First do cross entropy\n",
    "pre_model.compile(optimizer=optimizer,\n",
    "              loss=softmax_cross_entropy,\n",
    "              metrics=['accuracy'])\n",
    "pre_model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "\n",
    "# second do arcface \n",
    "modsoft_model.compile(optimizer=optimizer,\n",
    "              loss=softmax_cross_entropy,\n",
    "              metrics=['accuracy'])\n",
    "modsoft_model.fit(train_dataset, epochs=9)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=modsoft_model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_train[0:1000])\n",
    "plot_scatter(embeddings, y_train[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sphere Face\n",
    "\n",
    "$$\\text{soft}(x_i) = \\frac{exp(||x_i||\\text{cos}(\\psi(\\theta_{yi})))}{exp(||x_i||\\text{cos}(\\psi(\\theta_{yi}))) + \\sum_{j;j\\neq yi}  exp(||x_i||\\text{cos}(\\psi(\\theta_{j})))   }$$,\n",
    "\n",
    "where, $\\psi(\\theta) = -1^k \\text{cos}(m\\theta)-2k$.\n",
    "\n",
    "In this case, $theta \\in [\\frac{k\\pi}{m} , \\frac{(k+1)\\pi}{m}]$  $k \\in [0, m-1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2280: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 6s 5ms/step - loss: 0.2726 - accuracy: 0.9185\n",
      "Epoch 1/9\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 22.5869 - accuracy: 0.0013\n",
      "Epoch 2/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 22.3491 - accuracy: 9.4922e-05\n",
      "Epoch 3/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 22.3256 - accuracy: 0.0042\n",
      "Epoch 4/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 22.3206 - accuracy: 0.0093\n",
      "Epoch 5/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 22.3184 - accuracy: 0.0105\n",
      "Epoch 6/9\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 22.3161 - accuracy: 0.0097\n",
      "Epoch 7/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 22.3185 - accuracy: 0.0106\n",
      "Epoch 8/9\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 22.3156 - accuracy: 0.0122\n",
      "Epoch 9/9\n",
      "938/938 [==============================] - 6s 7ms/step - loss: 22.3103 - accuracy: 0.0113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc7971427004484898324292561eec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "\n",
    "M = 4\n",
    "\n",
    "class AngularHead(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_classes=10, scale=1.):\n",
    "        super(AngularHead, self).__init__(name =\"sphereface_logits\")\n",
    "        self.n_classes = n_classes\n",
    "        self.scale = scale\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(AngularHead, self).build(input_shape[0])\n",
    "        shape = [input_shape[-1], self.n_classes]\n",
    "        \n",
    "        self.W = self.add_variable(\"W\", shape=shape)\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        \n",
    "        # normalize feature\n",
    "        X_ = tf.nn.l2_normalize(X, axis=1)\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "\n",
    "        # cos between X and W            \n",
    "        logits = self.scale*(X_ @ W)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    \n",
    "def sphere_face_loss(target, output, sparse=True, n_classes=10, m=4):\n",
    "    \n",
    "    # The output is cos(theta)\n",
    "    \n",
    "    target = tf.squeeze(target)\n",
    "    pi = tf.constant(math.pi)\n",
    "    \n",
    "    if sparse:\n",
    "        y = tf.one_hot(tf.cast(target,\"int32\"), depth=n_classes)\n",
    "    else:\n",
    "        y = target\n",
    "\n",
    "    # Creating a mask to transforme the targets \n",
    "    # from [[1,0,..0]..] to [[0,1,..1]..]\n",
    "    mask = 1-y\n",
    "    \n",
    "    # cos(m*theta)\n",
    "    theta = tf.math.acos(output)    \n",
    "    cos_theta_m = tf.math.cos(m*theta)\n",
    "\n",
    "    # k [] \n",
    "    k = m * (theta / pi)\n",
    "        \n",
    "    # phi = -1**k * cos(m \\theta) - 2k\n",
    "    phi = (-1**k)* tf.math.cos(m*theta) - 2*k\n",
    "    \n",
    "    \n",
    "    # ||x||\n",
    "    x_norm = tf.norm(output, axis=-1, keepdims=True)\n",
    "\n",
    "    \n",
    "    # exp( ||x||cos(m \\theta) )\n",
    "    exp_x_cos_mtheta = x_norm * phi\n",
    "        \n",
    "    #  \\sum_{j;j\\neq yi}  exp(||x_i||\\text{cos}(\\psi(\\theta_{j})))\n",
    "    sum_exp_x_cos_mtheta = tf.math.reduce_sum(mask * exp_x_cos_mtheta, axis=-1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    mod_soft_max = exp_x_cos_mtheta / (exp_x_cos_mtheta + sum_exp_x_cos_mtheta)\n",
    "    \n",
    "\n",
    "    ### NOT SIMPLIFIED\n",
    "\n",
    "    mul = -tf.math.log(tf.clip_by_value(mod_soft_max,1e-10,1.0))\n",
    "    L = tf.math.reduce_mean(tf.math.reduce_sum(mul, axis=-1))\n",
    "\n",
    "    return L\n",
    "    \n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "\n",
    "# PRE MODEL WITH CROSS ENTROPY\n",
    "n_classes = 10\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"input\")\n",
    "labels = tf.keras.layers.Input([], name=\"label\")\n",
    "\n",
    "embeddings = backbone(inputs, include_top=False)\n",
    "logits_cross_entropy = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(embeddings)\n",
    "pre_model = tf.keras.Model(inputs=inputs, outputs=logits_cross_entropy)\n",
    "\n",
    "#### NOW THE MODIFIED CROSS ENTROPY\n",
    "\n",
    "logits_sphereface = AngularHead()(embeddings)\n",
    "\n",
    "sphereface_model = tf.keras.Model(inputs, outputs=logits_sphereface)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# First do cross entropy\n",
    "pre_model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "pre_model.fit(train_dataset, epochs=1)\n",
    "\n",
    "\n",
    "# second do arcface \n",
    "sphereface_model.compile(optimizer=optimizer,\n",
    "              loss=sphere_face_loss,\n",
    "              metrics=['accuracy'])\n",
    "sphereface_model.fit(train_dataset, epochs=9)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=sphereface_model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_test[0:1000])\n",
    "plot_scatter(embeddings, y_test[0:1000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace simple\n",
    "\n",
    "Essentially, in ArcFace, the logit is transform as follows $W^{\\intercal}x_i = ||W|| ||x_i|| cos(\\theta)$ where $\\theta$ is the angle between the weight $W$ and the feture $x_i$.\n",
    "Then basically the softmax-cors is replaced by:\n",
    "\n",
    "\n",
    "$$\\text{arc}(x_i) = \\frac{\\text{exp}(s(cos(\\theta_i) + m))}{\\text{exp}(s(cos(\\theta_i) + m)) + \\sum\\limits_{j=1;j\\neq i}^{m} \\text{exp}(s(cos(\\theta_j) + m))} $$,\n",
    "where $s$ is a scaling factor and $m$ is a margin penalty.\n",
    "\n",
    "Below follow the same example as aforementioned, but using the center loss.\n",
    "It's possible to observe from the t-SNE below that the representation for each one of the 10 classes is more \"visual\" compact (within class variability) compared with the cross-entropy example ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2280: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2726 - accuracy: 0.9185\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# ARCFACE HYPER PARAMETERS\n",
    "S = 30.\n",
    "M = 3.\n",
    "\n",
    "\n",
    "def arcface_loss(target, output, sparse=True, n_classes=10, m=5., s=30.):\n",
    "        \n",
    "    # output = s*cos(theta)    \n",
    "    cos_theta = output # just to wrap my head well.\n",
    "    cos_m = tf.math.cos(m)\n",
    "    sin_m = tf.math.sin(m)\n",
    "        \n",
    "    target = tf.squeeze(target)\n",
    "    \n",
    "    if sparse:\n",
    "        y = tf.one_hot(tf.cast(target,\"int32\"), depth=n_classes)\n",
    "    else:\n",
    "        y = target\n",
    "    \n",
    "    # Creating a mask to transforme the targets \n",
    "    # from [[1,0,..0]..] to [[0,1,..1]..]\n",
    "    mask = 1-y\n",
    "\n",
    "    # sin(x) = 1-cos(x)**2\n",
    "    sin_theta = tf.clip_by_value(tf.math.sqrt(1 - cos_theta ** 2), 0, 1)\n",
    "    \n",
    "    \n",
    "    # cos(x+m) = cos(x)*cos(m) - sin(x)*sin(m)\n",
    "    cos_theta_m = cos_theta * cos_m - sin_theta * sin_m    \n",
    "    \n",
    "    #exp(s*cos(theta))\n",
    "    exp_s_cos_theta = tf.math.exp(s*cos_theta)\n",
    "    \n",
    "    #exp(s* cos(theta + m))\n",
    "    exp_s_cos_theta_m = tf.math.exp(s*cos_theta_m)\n",
    "    \n",
    "    \n",
    "    #  \\sum\\limits_{j=1;j\\neq i}^{m} \\text{exp}(s(cos(\\theta_j) + m))\n",
    "    sum_exp_cos_theta = tf.math.reduce_sum(mask * exp_s_cos_theta, axis=-1, keepdims=True)\n",
    "    \n",
    "        \n",
    "    # Modified softmax    \n",
    "    mod_soft_max = (exp_s_cos_theta_m) / (exp_s_cos_theta_m + sum_exp_cos_theta)\n",
    "    \n",
    "\n",
    "    mul = -tf.math.log(tf.clip_by_value(mod_soft_max,1e-10,1.0))\n",
    "    L = tf.math.reduce_mean(tf.math.reduce_sum(mul, axis=-1))\n",
    "\n",
    "    return L        \n",
    "\n",
    "                    \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# PRE MODEL WITH CROSS ENTROPY\n",
    "n_classes = 10\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"input\")\n",
    "labels = tf.keras.layers.Input([], name=\"label\")\n",
    "\n",
    "embeddings = backbone(inputs, include_top=False)\n",
    "logits_cross_entropy = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(embeddings)\n",
    "pre_model = tf.keras.Model(inputs=inputs, outputs=logits_cross_entropy)\n",
    "#print(pre_model.summary())\n",
    "\n",
    "# ARC FACE MODEL\n",
    "logits_arcface = AngularHead()(embeddings)\n",
    "arcface_model = tf.keras.Model(inputs, outputs=logits_arcface)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# First do cross entropy\n",
    "pre_model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "pre_model.fit(train_dataset, epochs=1)\n",
    "\n",
    "\n",
    "# second do arcface\n",
    "#\n",
    "arcface_model.compile(optimizer=optimizer,\n",
    "              loss=partial(arcface_loss,m=M,s=S),\n",
    "              metrics=['accuracy'])\n",
    "arcface_model.fit(train_dataset, epochs=9)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=arcface_model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_test[0:1000])\n",
    "plot_scatter(embeddings, y_test[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace + Sphere Face + CosFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2280: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 6s 5ms/step - loss: 0.2726 - accuracy: 0.9185\n",
      "Epoch 1/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.1449\n",
      "Epoch 2/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 3/9\n",
      "938/938 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 4/9\n",
      "938/938 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 5/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 6/9\n",
      "938/938 [==============================] - 6s 7ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 7/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 8/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 9/9\n",
      "938/938 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.0987\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c62d3f6399d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mpredict_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marcface_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mplot_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/remote/idiap.svm/user.active/tpereira/github/paper_notes/notes/ml/arcface/plot.py\u001b[0m in \u001b[0;36mplot_scatter\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \"\"\"\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    669\u001b[0m             X = self._validate_data(X, accept_sparse=['csr'],\n\u001b[1;32m    670\u001b[0m                                     \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                                     dtype=[np.float32, np.float64])\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             X = self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'],\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 645\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/idiap/user/tpereira/conda/envs/bob.nightlies/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n\u001b[0;32m---> 99\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    100\u001b[0m             )\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# ARCFACE HYPER PARAMETERS\n",
    "S = 30.\n",
    "M1 = 1.\n",
    "M2 = 0.5\n",
    "M3 = 0.\n",
    "\n",
    "\n",
    "\n",
    "def arcface_three_penalties_loss(target, output, sparse=True, n_classes=10, m1=5., m2=5., m3=5., s=30.):\n",
    "        \n",
    "    # The output is cos(theta)    \n",
    "    cos_theta = output # just to wrap my head well.\n",
    "\n",
    "    # Getting the angle\n",
    "    theta = tf.math.acos(cos_theta)\n",
    "        \n",
    "    target = tf.squeeze(target)    \n",
    "    if sparse:\n",
    "        y = tf.one_hot(tf.cast(target,\"int32\"), depth=n_classes)\n",
    "    else:\n",
    "        y = target\n",
    "    \n",
    "    # Creating a mask to transforme the targets \n",
    "    # from [[1,0,..0]..] to [[0,1,..1]..]\n",
    "    mask = 1-y\n",
    "\n",
    "    \n",
    "    # exp(s * cos(\\theta))\n",
    "    exp_s_cos_theta = tf.math.exp(s*cos_theta)\n",
    "    \n",
    "    #exp(cos_theta + m)\n",
    "    exp_s_cos_theta_m = tf.math.exp( s* tf.math.cos(m1*theta+m2)-m3 )\n",
    "    \n",
    "    \n",
    "    #  \\sum\\limits_{j=1;j\\neq i}^{m} \\text{exp}(s(cos(\\theta_j) + m))\n",
    "    sum_exp_s_cos_theta = tf.math.reduce_sum(mask * exp_s_cos_theta, axis=-1, keepdims=True)\n",
    "    \n",
    "        \n",
    "    # Modified softmax    \n",
    "    mod_soft_max = (exp_s_cos_theta_m) / (exp_s_cos_theta_m + sum_exp_s_cos_theta)\n",
    "    \n",
    "\n",
    "    mul = -tf.math.log(tf.clip_by_value(mod_soft_max,1e-10,1.0))\n",
    "    L = tf.math.reduce_mean(tf.math.reduce_sum(mul, axis=-1))\n",
    "\n",
    "    return L        \n",
    "\n",
    "                    \n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.batch(64)\n",
    "\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "# PRE MODEL WITH CROSS ENTROPY\n",
    "n_classes = 10\n",
    "inputs = tf.keras.layers.Input([28, 28, 1], name=\"input\")\n",
    "labels = tf.keras.layers.Input([], name=\"label\")\n",
    "\n",
    "embeddings = backbone(inputs, include_top=False)\n",
    "logits_cross_entropy = tf.keras.layers.Dense(n_classes, name=\"hot\", activation=None)(embeddings)\n",
    "pre_model = tf.keras.Model(inputs=inputs, outputs=logits_cross_entropy)\n",
    "#print(pre_model.summary())\n",
    "\n",
    "# ARC FACE MODEL\n",
    "logits_arcface = AngularHead()(embeddings)\n",
    "arcface_model = tf.keras.Model(inputs, outputs=logits_arcface)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# First do cross entropy\n",
    "pre_model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "pre_model.fit(train_dataset, epochs=1)\n",
    "\n",
    "\n",
    "# second do arcface\n",
    "#\n",
    "arcface_model.compile(optimizer=optimizer,\n",
    "              loss=partial(arcface_three_penalties_loss, m1=M1, m2=M2, m3=M3, s=S),\n",
    "              metrics=['accuracy'])\n",
    "arcface_model.fit(train_dataset, epochs=9)\n",
    "\n",
    "\n",
    "predict_model = tf.keras.Model(inputs=inputs, outputs=arcface_model.get_layer(\"embeddings\").output)\n",
    "embeddings = predict_model.predict(x_test[0:1000])\n",
    "plot_scatter(embeddings, y_test[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
